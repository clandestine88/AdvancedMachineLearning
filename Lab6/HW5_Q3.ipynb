{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.datasets import reuters\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import numpy as np\n",
    "from keras import regularizers\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.set_printoptions(precision=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import the reuters data\n",
    "(input_train_data, input_train_labels), (input_test_data, input_test_labels) = reuters.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a function to one hot encode training data\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create the one hot train labels\n",
    "one_hot_train_labels = to_categorical(input_train_labels)\n",
    "one_hot_test_labels = to_categorical(input_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Our vectorized training data\n",
    "x_train = vectorize_sequences(input_train_data)\n",
    "x_val = x_train[:1000]\n",
    "partial_x_train = x_train[1000:]\n",
    "\n",
    "y_val = one_hot_train_labels[:1000]\n",
    "partial_y_train = one_hot_train_labels[1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a function to build a compiled model\n",
    "def build_model(lambda_val):\n",
    "    \n",
    "    # build an L2 regularizer\n",
    "    regularizer = keras.regularizers.l2(lambda_val)\n",
    "\n",
    "    # create the model adding the regularizer to each layer with 64 perceptrons\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(64, activation='relu', input_shape=(10000,), kernel_regularizer = regularizer))\n",
    "    model.add(layers.Dense(64, activation='relu', kernel_regularizer = regularizer))\n",
    "    model.add(layers.Dense(46, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='rmsprop', loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create an L2 lambda list\n",
    "lambda_list = [0, 0.001, 0.005, 0.0005]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================================================\n",
      "Fitting model with lambda =  0\n",
      "=====================================================================================\n",
      "Train on 7982 samples, validate on 1000 samples\n",
      "Epoch 1/20\n",
      "7982/7982 [==============================] - 2s 215us/step - loss: 2.5322 - acc: 0.4955 - val_loss: 1.7208 - val_acc: 0.6120\n",
      "Epoch 2/20\n",
      "7982/7982 [==============================] - 1s 157us/step - loss: 1.4452 - acc: 0.6879 - val_loss: 1.3459 - val_acc: 0.7060\n",
      "Epoch 3/20\n",
      "7982/7982 [==============================] - 1s 153us/step - loss: 1.0953 - acc: 0.7651 - val_loss: 1.1708 - val_acc: 0.7430\n",
      "Epoch 4/20\n",
      "7982/7982 [==============================] - 1s 154us/step - loss: 0.8697 - acc: 0.8165 - val_loss: 1.0793 - val_acc: 0.7590\n",
      "Epoch 5/20\n",
      "7982/7982 [==============================] - 1s 154us/step - loss: 0.7034 - acc: 0.8472 - val_loss: 0.9844 - val_acc: 0.7810\n",
      "Epoch 6/20\n",
      "7982/7982 [==============================] - 1s 155us/step - loss: 0.5667 - acc: 0.8802 - val_loss: 0.9411 - val_acc: 0.8040\n",
      "Epoch 7/20\n",
      "7982/7982 [==============================] - 1s 157us/step - loss: 0.4581 - acc: 0.9048 - val_loss: 0.9083 - val_acc: 0.8020\n",
      "Epoch 8/20\n",
      "7982/7982 [==============================] - 1s 155us/step - loss: 0.3695 - acc: 0.9231 - val_loss: 0.9363 - val_acc: 0.7890\n",
      "Epoch 9/20\n",
      "7982/7982 [==============================] - 1s 165us/step - loss: 0.3032 - acc: 0.9315 - val_loss: 0.8917 - val_acc: 0.8090\n",
      "Epoch 10/20\n",
      "7982/7982 [==============================] - 1s 164us/step - loss: 0.2537 - acc: 0.9414 - val_loss: 0.9071 - val_acc: 0.8110\n",
      "Epoch 11/20\n",
      "7982/7982 [==============================] - 1s 162us/step - loss: 0.2187 - acc: 0.9471 - val_loss: 0.9177 - val_acc: 0.8130\n",
      "Epoch 12/20\n",
      "7982/7982 [==============================] - 1s 159us/step - loss: 0.1873 - acc: 0.9508 - val_loss: 0.9027 - val_acc: 0.8130\n",
      "Epoch 13/20\n",
      "7982/7982 [==============================] - 1s 158us/step - loss: 0.1703 - acc: 0.9521 - val_loss: 0.9323 - val_acc: 0.8110\n",
      "Epoch 14/20\n",
      "7982/7982 [==============================] - 1s 156us/step - loss: 0.1536 - acc: 0.9554 - val_loss: 0.9689 - val_acc: 0.8050\n",
      "Epoch 15/20\n",
      "7982/7982 [==============================] - 1s 157us/step - loss: 0.1390 - acc: 0.9560 - val_loss: 0.9686 - val_acc: 0.8150\n",
      "Epoch 16/20\n",
      "7982/7982 [==============================] - 1s 159us/step - loss: 0.1313 - acc: 0.9560 - val_loss: 1.0220 - val_acc: 0.8060\n",
      "Epoch 17/20\n",
      "7982/7982 [==============================] - 1s 160us/step - loss: 0.1217 - acc: 0.9579 - val_loss: 1.0254 - val_acc: 0.7970\n",
      "Epoch 18/20\n",
      "7982/7982 [==============================] - 1s 158us/step - loss: 0.1198 - acc: 0.9582 - val_loss: 1.0430 - val_acc: 0.8060\n",
      "Epoch 19/20\n",
      "7982/7982 [==============================] - 1s 156us/step - loss: 0.1138 - acc: 0.9597 - val_loss: 1.0955 - val_acc: 0.7970\n",
      "Epoch 20/20\n",
      "7982/7982 [==============================] - 1s 157us/step - loss: 0.1111 - acc: 0.9593 - val_loss: 1.0674 - val_acc: 0.8020\n",
      "=====================================================================================\n",
      "Fitting model with lambda =  0.001\n",
      "=====================================================================================\n",
      "Train on 7982 samples, validate on 1000 samples\n",
      "Epoch 1/20\n",
      "7982/7982 [==============================] - 2s 220us/step - loss: 2.6962 - acc: 0.5213 - val_loss: 1.8373 - val_acc: 0.6470\n",
      "Epoch 2/20\n",
      "7982/7982 [==============================] - 1s 166us/step - loss: 1.5550 - acc: 0.7038 - val_loss: 1.4503 - val_acc: 0.7130\n",
      "Epoch 3/20\n",
      "7982/7982 [==============================] - 1s 162us/step - loss: 1.2240 - acc: 0.7682 - val_loss: 1.3199 - val_acc: 0.7460\n",
      "Epoch 4/20\n",
      "7982/7982 [==============================] - 1s 161us/step - loss: 1.0401 - acc: 0.8081 - val_loss: 1.2540 - val_acc: 0.7460\n",
      "Epoch 5/20\n",
      "7982/7982 [==============================] - 1s 162us/step - loss: 0.9148 - acc: 0.8395 - val_loss: 1.1882 - val_acc: 0.7900\n",
      "Epoch 6/20\n",
      "7982/7982 [==============================] - 1s 165us/step - loss: 0.8175 - acc: 0.8652 - val_loss: 1.1213 - val_acc: 0.8070\n",
      "Epoch 7/20\n",
      "7982/7982 [==============================] - 1s 161us/step - loss: 0.7423 - acc: 0.8850 - val_loss: 1.1086 - val_acc: 0.8170\n",
      "Epoch 8/20\n",
      "7982/7982 [==============================] - 1s 161us/step - loss: 0.6822 - acc: 0.9008 - val_loss: 1.0848 - val_acc: 0.8140\n",
      "Epoch 9/20\n",
      "7982/7982 [==============================] - 1s 161us/step - loss: 0.6387 - acc: 0.9093 - val_loss: 1.0892 - val_acc: 0.8130\n",
      "Epoch 10/20\n",
      "7982/7982 [==============================] - 1s 163us/step - loss: 0.5919 - acc: 0.9232 - val_loss: 1.0704 - val_acc: 0.8190\n",
      "Epoch 11/20\n",
      "7982/7982 [==============================] - 1s 166us/step - loss: 0.5606 - acc: 0.9291 - val_loss: 1.0827 - val_acc: 0.8180\n",
      "Epoch 12/20\n",
      "7982/7982 [==============================] - 1s 164us/step - loss: 0.5419 - acc: 0.9341 - val_loss: 1.0633 - val_acc: 0.8230\n",
      "Epoch 13/20\n",
      "7982/7982 [==============================] - 1s 165us/step - loss: 0.5136 - acc: 0.9405 - val_loss: 1.0952 - val_acc: 0.8110\n",
      "Epoch 14/20\n",
      "7982/7982 [==============================] - 1s 162us/step - loss: 0.4997 - acc: 0.9415 - val_loss: 1.0648 - val_acc: 0.8130\n",
      "Epoch 15/20\n",
      "7982/7982 [==============================] - 1s 165us/step - loss: 0.4834 - acc: 0.9442 - val_loss: 1.0765 - val_acc: 0.8200\n",
      "Epoch 16/20\n",
      "7982/7982 [==============================] - 1s 167us/step - loss: 0.4671 - acc: 0.9478 - val_loss: 1.0621 - val_acc: 0.8220\n",
      "Epoch 17/20\n",
      "7982/7982 [==============================] - 1s 162us/step - loss: 0.4604 - acc: 0.9489 - val_loss: 1.0668 - val_acc: 0.8210\n",
      "Epoch 18/20\n",
      "7982/7982 [==============================] - 1s 170us/step - loss: 0.4452 - acc: 0.9489 - val_loss: 1.0595 - val_acc: 0.8230\n",
      "Epoch 19/20\n",
      "7982/7982 [==============================] - 1s 171us/step - loss: 0.4403 - acc: 0.9500 - val_loss: 1.1190 - val_acc: 0.8030\n",
      "Epoch 20/20\n",
      "7982/7982 [==============================] - 1s 174us/step - loss: 0.4378 - acc: 0.9503 - val_loss: 1.0762 - val_acc: 0.8160\n",
      "=====================================================================================\n",
      "Fitting model with lambda =  0.005\n",
      "=====================================================================================\n",
      "Train on 7982 samples, validate on 1000 samples\n",
      "Epoch 1/20\n",
      "7982/7982 [==============================] - 2s 221us/step - loss: 3.4222 - acc: 0.4899 - val_loss: 2.4116 - val_acc: 0.6120\n",
      "Epoch 2/20\n",
      "7982/7982 [==============================] - 1s 163us/step - loss: 2.0747 - acc: 0.6688 - val_loss: 1.8822 - val_acc: 0.6610\n",
      "Epoch 3/20\n",
      "7982/7982 [==============================] - 1s 163us/step - loss: 1.7119 - acc: 0.7243 - val_loss: 1.6998 - val_acc: 0.7110\n",
      "Epoch 4/20\n",
      "7982/7982 [==============================] - 1s 165us/step - loss: 1.5493 - acc: 0.7612 - val_loss: 1.6017 - val_acc: 0.7330\n",
      "Epoch 5/20\n",
      "7982/7982 [==============================] - 1s 166us/step - loss: 1.4474 - acc: 0.7798 - val_loss: 1.5354 - val_acc: 0.7550\n",
      "Epoch 6/20\n",
      "7982/7982 [==============================] - 1s 163us/step - loss: 1.3646 - acc: 0.8019 - val_loss: 1.4976 - val_acc: 0.7680\n",
      "Epoch 7/20\n",
      "7982/7982 [==============================] - 1s 164us/step - loss: 1.3082 - acc: 0.8107 - val_loss: 1.4585 - val_acc: 0.7810\n",
      "Epoch 8/20\n",
      "7982/7982 [==============================] - 1s 164us/step - loss: 1.2573 - acc: 0.8231 - val_loss: 1.4460 - val_acc: 0.7830\n",
      "Epoch 9/20\n",
      "7982/7982 [==============================] - 1s 167us/step - loss: 1.2178 - acc: 0.8287 - val_loss: 1.4150 - val_acc: 0.7920\n",
      "Epoch 10/20\n",
      "7982/7982 [==============================] - 1s 163us/step - loss: 1.1907 - acc: 0.8320 - val_loss: 1.4073 - val_acc: 0.7880\n",
      "Epoch 11/20\n",
      "7982/7982 [==============================] - 1s 164us/step - loss: 1.1557 - acc: 0.8371 - val_loss: 1.4029 - val_acc: 0.7890\n",
      "Epoch 12/20\n",
      "7982/7982 [==============================] - 1s 164us/step - loss: 1.1346 - acc: 0.8425 - val_loss: 1.3919 - val_acc: 0.7940\n",
      "Epoch 13/20\n",
      "7982/7982 [==============================] - 1s 165us/step - loss: 1.1193 - acc: 0.8434 - val_loss: 1.3646 - val_acc: 0.8010\n",
      "Epoch 14/20\n",
      "7982/7982 [==============================] - 1s 173us/step - loss: 1.0912 - acc: 0.8519 - val_loss: 1.3549 - val_acc: 0.8020\n",
      "Epoch 15/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7982/7982 [==============================] - 1s 163us/step - loss: 1.0763 - acc: 0.8530 - val_loss: 1.3481 - val_acc: 0.7930\n",
      "Epoch 16/20\n",
      "7982/7982 [==============================] - 1s 164us/step - loss: 1.0474 - acc: 0.8612 - val_loss: 1.4025 - val_acc: 0.7790\n",
      "Epoch 17/20\n",
      "7982/7982 [==============================] - 1s 164us/step - loss: 1.0434 - acc: 0.8646 - val_loss: 1.3539 - val_acc: 0.7890\n",
      "Epoch 18/20\n",
      "7982/7982 [==============================] - 1s 164us/step - loss: 1.0195 - acc: 0.8710 - val_loss: 1.3539 - val_acc: 0.7960\n",
      "Epoch 19/20\n",
      "7982/7982 [==============================] - 1s 167us/step - loss: 1.0067 - acc: 0.8717 - val_loss: 1.3181 - val_acc: 0.7940\n",
      "Epoch 20/20\n",
      "7982/7982 [==============================] - 1s 164us/step - loss: 0.9954 - acc: 0.8733 - val_loss: 1.3392 - val_acc: 0.8040\n",
      "=====================================================================================\n",
      "Fitting model with lambda =  0.0005\n",
      "=====================================================================================\n",
      "Train on 7982 samples, validate on 1000 samples\n",
      "Epoch 1/20\n",
      "7982/7982 [==============================] - 2s 234us/step - loss: 2.9276 - acc: 0.4043 - val_loss: 2.0284 - val_acc: 0.6260\n",
      "Epoch 2/20\n",
      "7982/7982 [==============================] - 1s 163us/step - loss: 1.6635 - acc: 0.6857 - val_loss: 1.4737 - val_acc: 0.7120\n",
      "Epoch 3/20\n",
      "7982/7982 [==============================] - 1s 168us/step - loss: 1.2364 - acc: 0.7618 - val_loss: 1.2847 - val_acc: 0.7380\n",
      "Epoch 4/20\n",
      "7982/7982 [==============================] - 1s 163us/step - loss: 1.0109 - acc: 0.8131 - val_loss: 1.1623 - val_acc: 0.7790\n",
      "Epoch 5/20\n",
      "7982/7982 [==============================] - 1s 165us/step - loss: 0.8601 - acc: 0.8418 - val_loss: 1.1039 - val_acc: 0.7940\n",
      "Epoch 6/20\n",
      "7982/7982 [==============================] - 1s 162us/step - loss: 0.7365 - acc: 0.8723 - val_loss: 1.0705 - val_acc: 0.7980\n",
      "Epoch 7/20\n",
      "7982/7982 [==============================] - 1s 164us/step - loss: 0.6509 - acc: 0.8935 - val_loss: 1.0627 - val_acc: 0.8070\n",
      "Epoch 8/20\n",
      "7982/7982 [==============================] - 1s 165us/step - loss: 0.5842 - acc: 0.9069 - val_loss: 1.0273 - val_acc: 0.8050\n",
      "Epoch 9/20\n",
      "7982/7982 [==============================] - 1s 163us/step - loss: 0.5312 - acc: 0.9211 - val_loss: 1.0264 - val_acc: 0.8140\n",
      "Epoch 10/20\n",
      "7982/7982 [==============================] - 1s 163us/step - loss: 0.4883 - acc: 0.9292 - val_loss: 1.0201 - val_acc: 0.8180\n",
      "Epoch 11/20\n",
      "7982/7982 [==============================] - 1s 164us/step - loss: 0.4541 - acc: 0.9380 - val_loss: 1.0123 - val_acc: 0.8140\n",
      "Epoch 12/20\n",
      "7982/7982 [==============================] - 1s 166us/step - loss: 0.4320 - acc: 0.9401 - val_loss: 1.0305 - val_acc: 0.8170\n",
      "Epoch 13/20\n",
      "7982/7982 [==============================] - 1s 168us/step - loss: 0.4065 - acc: 0.9458 - val_loss: 1.0139 - val_acc: 0.8140\n",
      "Epoch 14/20\n",
      "7982/7982 [==============================] - 1s 162us/step - loss: 0.3925 - acc: 0.9459 - val_loss: 1.0657 - val_acc: 0.8080\n",
      "Epoch 15/20\n",
      "7982/7982 [==============================] - 1s 163us/step - loss: 0.3746 - acc: 0.9484 - val_loss: 1.0354 - val_acc: 0.8050\n",
      "Epoch 16/20\n",
      "7982/7982 [==============================] - 1s 164us/step - loss: 0.3639 - acc: 0.9506 - val_loss: 1.0195 - val_acc: 0.8190\n",
      "Epoch 17/20\n",
      "7982/7982 [==============================] - 1s 166us/step - loss: 0.3545 - acc: 0.9526 - val_loss: 1.0443 - val_acc: 0.8120\n",
      "Epoch 18/20\n",
      "7982/7982 [==============================] - 1s 166us/step - loss: 0.3463 - acc: 0.9518 - val_loss: 1.0446 - val_acc: 0.8120\n",
      "Epoch 19/20\n",
      "7982/7982 [==============================] - 1s 164us/step - loss: 0.3389 - acc: 0.9535 - val_loss: 1.0393 - val_acc: 0.8120\n",
      "Epoch 20/20\n",
      "7982/7982 [==============================] - 1s 164us/step - loss: 0.3227 - acc: 0.9555 - val_loss: 1.0234 - val_acc: 0.8160\n"
     ]
    }
   ],
   "source": [
    "# for each L2 lambda value\n",
    "for lambda_val in lambda_list:\n",
    "\n",
    "    # get a compiled model\n",
    "    model = build_model(lambda_val)\n",
    "\n",
    "    # Print the lambda used for the model\n",
    "    print(\"=====================================================================================\")\n",
    "    print(\"Fitting model with lambda = \", lambda_val)\n",
    "    print(\"=====================================================================================\")\n",
    "\n",
    "    # train the model using the training data fold\n",
    "    history = model.fit(partial_x_train, partial_y_train,epochs=20, batch_size=512, validation_data=(x_val, y_val))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
